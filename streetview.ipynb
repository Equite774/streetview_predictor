{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-17T18:17:49.594201Z","iopub.status.busy":"2024-10-17T18:17:49.593396Z","iopub.status.idle":"2024-10-17T18:17:56.871887Z","shell.execute_reply":"2024-10-17T18:17:56.870801Z","shell.execute_reply.started":"2024-10-17T18:17:49.594142Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/chaiharsha/Documents/streetview_predictor/streetviewenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/chaiharsha/Documents/streetview_predictor/streetviewenv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n","  Referenced from: <61623A3D-DA3C-3AAD-B2F0-D363151DDB3F> /Users/chaiharsha/Documents/streetview_predictor/streetviewenv/lib/python3.10/site-packages/torchvision/image.so\n","  Expected in:     <66FB8649-BB87-3CD6-A177-462038DCAE02> /Users/chaiharsha/Documents/streetview_predictor/streetviewenv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch # deep learning\n","from torchvision import transforms # image preprocessing\n","from PIL import Image # read image\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:25:21.076929Z","iopub.status.busy":"2024-10-17T18:25:21.075834Z","iopub.status.idle":"2024-10-17T18:25:21.085091Z","shell.execute_reply":"2024-10-17T18:25:21.083367Z","shell.execute_reply.started":"2024-10-17T18:25:21.076863Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20.824885</td>\n","      <td>-98.499517</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-3.451752</td>\n","      <td>-54.563937</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-23.496464</td>\n","      <td>-47.460542</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-16.548678</td>\n","      <td>-72.852778</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-35.010870</td>\n","      <td>140.064397</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    latitude   longitude\n","0  20.824885  -98.499517\n","1  -3.451752  -54.563937\n","2 -23.496464  -47.460542\n","3 -16.548678  -72.852778\n","4 -35.010870  140.064397"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Get labels\n","df = pd.read_csv(os.path.join('coordinates.csv'))\n","df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# define the image preprocessing transformation\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize to a fixed size\n","    transforms.ToTensor(),          # Convert image to tensor\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize the image\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'Streetview_Image_Dataset/0.jpeg'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m coords\u001b[38;5;241m.\u001b[39mappend(coordinate)\n\u001b[1;32m      8\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreetview_Image_Dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Image filename based on 'num' column\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[1;32m     11\u001b[0m images\u001b[38;5;241m.\u001b[39mappend(image)\n","File \u001b[0;32m~/Documents/streetview_predictor/streetviewenv/lib/python3.10/site-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Streetview_Image_Dataset/0.jpeg'"]}],"source":["coords = []\n","images = []\n","\n","for i, row in df.iterrows():\n","    coordinate = row[['latitude', 'longitude']].values.astype('float32')\n","    coords.append(coordinate)\n","    \n","    img_name = f\"Streetview_Image_Dataset/{i}.png\"  # Image filename based on 'num' column\n","    image = Image.open(img_name)\n","    image = transform(image)\n","    images.append(image)\n","    if i % 1000 == 0:\n","        print(f\"Processed {i} images\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:31:04.028960Z","iopub.status.busy":"2024-10-17T18:31:04.028434Z","iopub.status.idle":"2024-10-17T18:31:04.448464Z","shell.execute_reply":"2024-10-17T18:31:04.446975Z","shell.execute_reply.started":"2024-10-17T18:31:04.028912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([40229, 3, 224, 224])\n","torch.Size([40229, 2])\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/8v/z7_lynrn24bbpwy6p36j3cwc0000gn/T/ipykernel_59942/539367343.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n","  labels = torch.tensor(coords)\n"]}],"source":["# Convert to torch tensors\n","data = torch.stack(images)\n","labels = torch.tensor(coords)\n","\n","# Ensure the shapes are correct\n","print(data.shape)\n","print(labels.shape)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["dataset = list(zip(data, labels))\n","\n","# define the sizes for training, validation, and test sets\n","train_size = int(0.8 * len(dataset))\n","val_size = int(0.1 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","# randomly split the dataset\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# create DataLoaders\n","batch_size = 64\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","# model\n","model = nn.Sequential(\n","    # 3x224x224\n","    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n","    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","    nn.Dropout(0.2),\n","    # 64x112x112\n","    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","    nn.Dropout(0.2),\n","    # 128x56x56\n","    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n","    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","    # 256x28x28\n","    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n","    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","    # 512x14x14\n","    nn.Flatten(),\n","    # 100352\n","    nn.Linear(100352, 4096),\n","    nn.ReLU(),\n","    nn.Dropout(0.35),\n","    # 4096\n","    nn.Linear(4096, 1024),\n","    nn.ReLU(),\n","    # 1024\n","    nn.Linear(1024, 256),\n","    nn.ReLU(),\n","    # 256\n","    nn.Linear(256, 2)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["ideas:\n","\n","make more channels\n","add some more conv2d layers\n","switch back to regular ReLU\n","\n","image augmentation\n","\n","image blackout"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# loss function\n","criterion = nn.MSELoss()\n","\n","# optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/12, Train Loss: 4122.220004678958\n","Epoch 1/12, Validation Loss: 3218.5552649119545\n","Epoch 2/12, Train Loss: 15592497.4627654\n","Epoch 2/12, Validation Loss: 3517.4995775979664\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/Documents/streetview_model/streetviewenv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/streetview_model/streetviewenv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# training + validation loop\n","num_epochs = 12\n","for epoch in range(num_epochs):\n","    # training\n","    model.train()\n","    running_train_loss = 0.0\n","    \n","    for images, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_train_loss += loss.item()\n","    \n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_train_loss/len(train_loader)}\")\n","\n","    # validation\n","    model.eval()\n","    running_val_loss = 0.0\n","    \n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            val_loss = criterion(outputs, labels)\n","            running_val_loss += val_loss.item()\n","    \n","    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {running_val_loss/len(val_loader)}\")\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# save the model\n","torch.save(model.state_dict(), 'model1017.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# testing loop\n","model.eval()  \n","running_test_loss = 0.0\n","all_predictions = []\n","all_true_values = []\n","all_indices = []\n","\n","with torch.no_grad(): \n","    for batch_idx, (images, labels) in enumerate(test_loader):\n","        outputs = model(images)\n","        test_loss = criterion(outputs, labels)\n","        running_test_loss += test_loss.item()\n","        \n","        # predictions and true values\n","        all_predictions.append(outputs.cpu().numpy())  \n","        all_true_values.append(labels.cpu().numpy()) \n","        \n","        # indices\n","        indices = test_loader.dataset.indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n","        all_indices.append(indices)\n","        \n","print(f\"Test Loss after training: {running_test_loss / len(test_loader)}\")\n","\n","# predictions\n","all_predictions = np.concatenate(all_predictions, axis=0)\n","all_true_values = np.concatenate(all_true_values, axis=0)\n","all_indices = np.concatenate(all_indices, axis=0)\n","\n","# samples\n","num_samples_to_print = 5 \n","for i in range(num_samples_to_print):\n","    print(f\"Sample {i + 1}: Dataset Index: {all_indices[i]}, True: {all_true_values[i]}, Predicted: {all_predictions[i]}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3352794,"sourceId":5832670,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":4}
